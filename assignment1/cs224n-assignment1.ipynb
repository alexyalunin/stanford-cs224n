{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/2.png\", width=\"150\", style=\"float: left\"><br><br><br>\n",
    "<img src=\"img/3.png\", width=\"200\", style=\"float: left\"><br><br><br>\n",
    "<img src=\"img/1.png\", width=\"600\", style=\"float: left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    D-dimensional vector (treat the vector as a single row) and\n",
    "    for N x D matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A D dimensional vector or N x D dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        exp_minmax = lambda x: np.exp(x - np.max(x))\n",
    "        denom = lambda x: 1.0 / np.sum(x)\n",
    "        x = np.apply_along_axis(exp_minmax, 1, x)\n",
    "        denominator = np.apply_along_axis(denom, 1, x)\n",
    "        \n",
    "        if len(denominator.shape) == 1:\n",
    "            denominator = denominator.reshape((denominator.shape[0], 1))\n",
    "\n",
    "        x = x * denominator\n",
    "\n",
    "    else:\n",
    "        # Vector\n",
    "        x_max = np.max(x)\n",
    "        x = x - x_max\n",
    "        numerator = np.exp(x)\n",
    "        denominator = 1.0 / np.sum(numerator)\n",
    "        x = numerator.dot(denominator)\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    ans1 = np.array([0.26894142,  0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print \"You should be able to verify these results by hand!\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[0.26894142 0.73105858]\n",
      "[[0.26894142 0.73105858]\n",
      " [0.26894142 0.73105858]]\n",
      "[[0.73105858 0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Neural Network Basics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/4.png\", width=\"150\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/5.png\", width=\"200\", style=\"float: left\"><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    s = 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = s * (1 - s)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print f\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print g\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"You should verify these results by hand!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sigmoid_basic();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        # Try modifying x[ix] with h defined above to compute numerical\n",
    "        # gradients (numgrad).\n",
    "\n",
    "        # Use the centered difference of the gradient.\n",
    "        # It has smaller asymptotic error than forward / backward difference\n",
    "        # methods. If you are curious, check out here:\n",
    "        # https://math.stackexchange.com/questions/2326181/when-to-use-forward-or-central-difference-approximations\n",
    "\n",
    "        # Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later.\n",
    "        \n",
    "        x[ix] += h\n",
    "        random.setstate(rndstate)\n",
    "        new_f1 = f(x)[0]\n",
    "        x[ix] -= 2*h\n",
    "        random.setstate(rndstate)\n",
    "        new_f2 = f(x)[0]\n",
    "        x[ix] += h\n",
    "\n",
    "        numgrad = (new_f1 - new_f2) / (2 * h)\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad)\n",
    "            return\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6.png\", width=\"125\", style=\"float: left\"><br><br>\n",
    "<img src=\"img/7.png\", width=\"225\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/8.png\", width=\"200\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/9.png\", width=\"300\", style=\"float: left\"><br><br><br><br><br><br><br><br><br><br>\n",
    "<img src=\"img/10.png\", width=\"500\", style=\"float: left\"><br><br><br>\n",
    "<img src=\"img/11.png\", width=\"500\", style=\"float: left\"><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    h = sigmoid(np.dot(data,W1) + b1)\n",
    "    yhat = softmax(np.dot(h,W2) + b2)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    cost = np.sum(-np.log(yhat[labels==1])) / data.shape[0]\n",
    "\n",
    "    d3 = (yhat - labels) / data.shape[0]\n",
    "    gradW2 = np.dot(h.T, d3)\n",
    "    gradb2 = np.sum(d3,0,keepdims=True)\n",
    "\n",
    "    dh = np.dot(d3,W2.T)\n",
    "    grad_h = sigmoid_grad(h) * dh\n",
    "\n",
    "    gradW1 = np.dot(data.T,grad_h)\n",
    "    gradb1 = np.sum(grad_h,0)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  3 word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    x /= np.sqrt(np.sum(np.square(x), axis=1, keepdims=True))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0, 4.0], [1, 2]]))\n",
    "    ans = np.array([[0.6, 0.8], [0.4472136, 0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/12.png\", width=\"250\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/13.png\", width=\"200\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/14.png\", width=\"150\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/15.png\", width=\"150\", style=\"float: left\"><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "\n",
    "    ## Gradient for $\\hat{\\bm{v}}$:\n",
    "\n",
    "    #  Calculate the predictions:\n",
    "    vc = predicted\n",
    "    U = outputVectors\n",
    "    y_hat = softmax( np.dot(U, vc) )\n",
    "\n",
    "    #  Calculate the cost:\n",
    "    cost = -np.log(y_hat[target])\n",
    "\n",
    "    #  Gradients\n",
    "    z = y_hat.copy()\n",
    "    z[target] -= 1.0\n",
    "\n",
    "    gradPred = np.dot(U.T, z) #3a\n",
    "    grad = np.outer(z, vc) #3b\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in xrange(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/16.png\", width=\"450\", style=\"float: left\"><br><br>\n",
    "<img src=\"img/17.png\", width=\"400\", style=\"float: left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    vc = predicted\n",
    "    U = outputVectors\n",
    "    \n",
    "    grad = np.zeros(U.shape)\n",
    "    gradPred = np.zeros(vc.shape)\n",
    "    cost = 0\n",
    "    z = sigmoid(np.dot(U[target], vc))\n",
    "\n",
    "    cost -= np.log(z)\n",
    "    gradPred += U[target] * (z - 1.0)\n",
    "    grad[target] += vc * (z - 1.0)\n",
    "\n",
    "    for k in xrange(K):\n",
    "        samp = indices[k + 1]\n",
    "        z = sigmoid(np.dot(U[samp], vc))\n",
    "        cost -= np.log(1.0 - z)   # sigmoid(-x) = 1 - sigmoid(x)\n",
    "        gradPred += U[samp] * z   # the same\n",
    "        grad[samp] += vc * z      # the same\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the center word is c = wt and the context words are [wt−m, . . ., wt−1, wt, wt+1,\n",
    ". . ., wt+m], where m is the context size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/18.png\", width=\"400\", style=\"float: left\"><br><br>\n",
    "<img src=\"img/19.png\", width=\"400\", style=\"float: left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "    Implement the skip-gram model in this function.\n",
    "    Arguments:\n",
    "    currrentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    cword_idx = tokens[currentWord]\n",
    "    vhat = inputVectors[cword_idx]\n",
    "\n",
    "    for j in contextWords:\n",
    "        u_idx = tokens[j]\n",
    "        c_cost, c_grad_in, c_grad_out = \\\n",
    "            word2vecCostAndGradient(vhat, u_idx, outputVectors, dataset)\n",
    "        cost += c_cost\n",
    "        gradIn[cword_idx] += c_grad_in\n",
    "        gradOut += c_grad_out\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/20.png\", width=\"200\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/21.png\", width=\"250\", style=\"float: left\"><br><br><br><br>\n",
    "<img src=\"img/22.png\", width=\"600\", style=\"float: left\"><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    predicted_indices = [tokens[word] for word in contextWords]\n",
    "    predicted_vectors = inputVectors[predicted_indices]\n",
    "    predicted = np.sum(predicted_vectors, axis=0)\n",
    "    target = tokens[currentWord]\n",
    "    cost, gradIn_predicted, gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n",
    "    for i in predicted_indices:\n",
    "        gradIn[i] += gradIn_predicted\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N / 2, :]\n",
    "    outputVectors = wordVectors[N / 2:, :]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1, C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N / 2, :] += gin / batchsize / denom\n",
    "        grad[N / 2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0, 4)], \\\n",
    "               [tokens[random.randint(0, 4)] for i in xrange(2 * C)]\n",
    "\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "    dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "                    dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "                   dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n",
    "                   negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "               dummy_tokens, dummy_vectors[:5, :], dummy_vectors[5:, :], dataset,\n",
    "               negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.16610900153398, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(13.959405258751875, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.12113804, -1.67347865, -1.5049951 ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.49853822,  0.22876535,  1.74016407],\n",
      "       [-0.02700439,  0.01239157,  0.09425972],\n",
      "       [-0.68292656,  0.31337605,  2.38377767],\n",
      "       [-0.84273629,  0.3867083 ,  2.94159878],\n",
      "       [-0.16124059,  0.07398883,  0.5628156 ]]))\n",
      "(0.798995801090665, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.763088992861874, array([[-3.24112111, -1.89068433, -2.69507064],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-1.55404334, -0.42160121,  1.03844397],\n",
      "       [-1.72636934, -0.46835207,  1.15359577],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "test_normalize_rows()\n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a cost and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    expcost = None\n",
    "\n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        # Don't forget to apply the postprocessing after every iteration!\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        cost, grad = f(x)\n",
    "        x -= step * grad\n",
    "        postprocessing(x)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print \"iter %d: %f\" % (iter, expcost)\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 1 result:\", t1\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 2 result:\", t2\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 3 result:\", t3\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.41483678608e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.52445103582e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "class StanfordSentiment:\n",
    "    def __init__(self, path=None, tablesize = 1000000):\n",
    "        if not path:\n",
    "            path = \"utils/datasets/stanfordSentimentTreebank\"\n",
    "\n",
    "        self.path = path\n",
    "        self.tablesize = tablesize\n",
    "\n",
    "    def tokens(self):\n",
    "        if hasattr(self, \"_tokens\") and self._tokens:\n",
    "            return self._tokens\n",
    "\n",
    "        tokens = dict()\n",
    "        tokenfreq = dict()\n",
    "        wordcount = 0\n",
    "        revtokens = []\n",
    "        idx = 0\n",
    "\n",
    "        for sentence in self.sentences():\n",
    "            for w in sentence:\n",
    "                wordcount += 1\n",
    "                if not w in tokens:\n",
    "                    tokens[w] = idx\n",
    "                    revtokens += [w]\n",
    "                    tokenfreq[w] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    tokenfreq[w] += 1\n",
    "\n",
    "        tokens[\"UNK\"] = idx\n",
    "        revtokens += [\"UNK\"]\n",
    "        tokenfreq[\"UNK\"] = 1\n",
    "        wordcount += 1\n",
    "\n",
    "        self._tokens = tokens\n",
    "        self._tokenfreq = tokenfreq\n",
    "        self._wordcount = wordcount\n",
    "        self._revtokens = revtokens\n",
    "        return self._tokens\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"_sentences\") and self._sentences:\n",
    "            return self._sentences\n",
    "\n",
    "        sentences = []\n",
    "        with open(self.path + \"/datasetSentences.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split()[1:]\n",
    "                # Deal with some peculiar encoding issues with this file\n",
    "                sentences += [[w.lower().decode(\"utf-8\").encode('latin1') for w in splitted]]\n",
    "\n",
    "        self._sentences = sentences\n",
    "        self._sentlengths = np.array([len(s) for s in sentences])\n",
    "        self._cumsentlen = np.cumsum(self._sentlengths)\n",
    "\n",
    "        return self._sentences\n",
    "\n",
    "    def numSentences(self):\n",
    "        if hasattr(self, \"_numSentences\") and self._numSentences:\n",
    "            return self._numSentences\n",
    "        else:\n",
    "            self._numSentences = len(self.sentences())\n",
    "            return self._numSentences\n",
    "\n",
    "    def allSentences(self):\n",
    "        if hasattr(self, \"_allsentences\") and self._allsentences:\n",
    "            return self._allsentences\n",
    "\n",
    "        sentences = self.sentences()\n",
    "        rejectProb = self.rejectProb()\n",
    "        tokens = self.tokens()\n",
    "        allsentences = [[w for w in s\n",
    "            if 0 >= rejectProb[tokens[w]] or random.random() >= rejectProb[tokens[w]]]\n",
    "            for s in sentences * 30]\n",
    "\n",
    "        allsentences = [s for s in allsentences if len(s) > 1]\n",
    "\n",
    "        self._allsentences = allsentences\n",
    "\n",
    "        return self._allsentences\n",
    "\n",
    "    def getRandomContext(self, C=5):\n",
    "        allsent = self.allSentences()\n",
    "        sentID = random.randint(0, len(allsent) - 1)\n",
    "        sent = allsent[sentID]\n",
    "        wordID = random.randint(0, len(sent) - 1)\n",
    "\n",
    "        context = sent[max(0, wordID - C):wordID]\n",
    "        if wordID+1 < len(sent):\n",
    "            context += sent[wordID+1:min(len(sent), wordID + C + 1)]\n",
    "\n",
    "        centerword = sent[wordID]\n",
    "        context = [w for w in context if w != centerword]\n",
    "\n",
    "        if len(context) > 0:\n",
    "            return centerword, context\n",
    "        else:\n",
    "            return self.getRandomContext(C)\n",
    "\n",
    "    def sent_labels(self):\n",
    "        if hasattr(self, \"_sent_labels\") and self._sent_labels:\n",
    "            return self._sent_labels\n",
    "\n",
    "        dictionary = dict()\n",
    "        phrases = 0\n",
    "        with open(self.path + \"/dictionary.txt\", \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "                phrases += 1\n",
    "\n",
    "        labels = [0.0] * phrases\n",
    "        with open(self.path + \"/sentiment_labels.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                splitted = line.split(\"|\")\n",
    "                labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "        sent_labels = [0.0] * self.numSentences()\n",
    "        sentences = self.sentences()\n",
    "        for i in xrange(self.numSentences()):\n",
    "            sentence = sentences[i]\n",
    "            full_sent = \" \".join(sentence).replace('-lrb-', '(').replace('-rrb-', ')')\n",
    "            sent_labels[i] = labels[dictionary[full_sent]]\n",
    "\n",
    "        self._sent_labels = sent_labels\n",
    "        return self._sent_labels\n",
    "\n",
    "    def dataset_split(self):\n",
    "        if hasattr(self, \"_split\") and self._split:\n",
    "            return self._split\n",
    "\n",
    "        split = [[] for i in xrange(3)]\n",
    "        with open(self.path + \"/datasetSplit.txt\", \"r\") as f:\n",
    "            first = True\n",
    "            for line in f:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    continue\n",
    "\n",
    "                splitted = line.strip().split(\",\")\n",
    "                split[int(splitted[1]) - 1] += [int(splitted[0]) - 1]\n",
    "\n",
    "        self._split = split\n",
    "        return self._split\n",
    "\n",
    "    def getRandomTrainSentence(self):\n",
    "        split = self.dataset_split()\n",
    "        sentId = split[0][random.randint(0, len(split[0]) - 1)]\n",
    "        return self.sentences()[sentId], self.categorify(self.sent_labels()[sentId])\n",
    "\n",
    "    def categorify(self, label):\n",
    "        if label <= 0.2:\n",
    "            return 0\n",
    "        elif label <= 0.4:\n",
    "            return 1\n",
    "        elif label <= 0.6:\n",
    "            return 2\n",
    "        elif label <= 0.8:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "    def getDevSentences(self):\n",
    "        return self.getSplitSentences(2)\n",
    "\n",
    "    def getTestSentences(self):\n",
    "        return self.getSplitSentences(1)\n",
    "\n",
    "    def getTrainSentences(self):\n",
    "        return self.getSplitSentences(0)\n",
    "\n",
    "    def getSplitSentences(self, split=0):\n",
    "        ds_split = self.dataset_split()\n",
    "        return [(self.sentences()[i], self.categorify(self.sent_labels()[i])) for i in ds_split[split]]\n",
    "\n",
    "    def sampleTable(self):\n",
    "        if hasattr(self, '_sampleTable') and self._sampleTable is not None:\n",
    "            return self._sampleTable\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        samplingFreq = np.zeros((nTokens,))\n",
    "        self.allSentences()\n",
    "        i = 0\n",
    "        for w in xrange(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            if w in self._tokenfreq:\n",
    "                freq = 1.0 * self._tokenfreq[w]\n",
    "                # Reweigh\n",
    "                freq = freq ** 0.75\n",
    "            else:\n",
    "                freq = 0.0\n",
    "            samplingFreq[i] = freq\n",
    "            i += 1\n",
    "\n",
    "        samplingFreq /= np.sum(samplingFreq)\n",
    "        samplingFreq = np.cumsum(samplingFreq) * self.tablesize\n",
    "\n",
    "        self._sampleTable = [0] * self.tablesize\n",
    "\n",
    "        j = 0\n",
    "        for i in xrange(self.tablesize):\n",
    "            while i > samplingFreq[j]:\n",
    "                j += 1\n",
    "            self._sampleTable[i] = j\n",
    "\n",
    "        return self._sampleTable\n",
    "\n",
    "    def rejectProb(self):\n",
    "        if hasattr(self, '_rejectProb') and self._rejectProb is not None:\n",
    "            return self._rejectProb\n",
    "\n",
    "        threshold = 1e-5 * self._wordcount\n",
    "\n",
    "        nTokens = len(self.tokens())\n",
    "        rejectProb = np.zeros((nTokens,))\n",
    "        for i in xrange(nTokens):\n",
    "            w = self._revtokens[i]\n",
    "            freq = 1.0 * self._tokenfreq[w]\n",
    "            # Reweigh\n",
    "            rejectProb[i] = max(0, 1 - np.sqrt(threshold / freq))\n",
    "\n",
    "        self._rejectProb = rejectProb\n",
    "        return self._rejectProb\n",
    "\n",
    "    def sampleTokenIdx(self):\n",
    "        return self.sampleTable()[random.randint(0, self.tablesize - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00070601 -0.03129069  0.03030657 ... -0.0420245  -0.0085924\n",
      "   0.01075564]\n",
      " [-0.0163571  -0.00571786  0.02621135 ... -0.00151971  0.02935297\n",
      "   0.00705453]\n",
      " [ 0.04588673 -0.04559    -0.01696984 ... -0.0047359  -0.01434289\n",
      "  -0.01258177]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 1 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "print \"training took %d seconds\" % (time.time() - startTime)\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n",
    "\n",
    "visualizeWords = [\n",
    "    \"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\",\n",
    "    \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\",\n",
    "    \"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/25.png\"><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.27960715, -0.13004219,  0.24583396, -0.55278474, -1.011707  ,\n",
       "        0.25111421,  0.24456245, -0.04147987,  0.39738506,  0.14428541])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors[tokens[\"bad\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getArguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--pretrained\", dest=\"pretrained\", action=\"store_true\",\n",
    "                       help=\"Use pretrained GloVe vectors.\")\n",
    "    group.add_argument(\"--yourvectors\", dest=\"yourvectors\", action=\"store_true\",\n",
    "                       help=\"Use your vectors from q3.\")\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentenceFeatures(tokens, wordVectors, sentence):\n",
    "    \"\"\"\n",
    "    Obtain the sentence feature for sentiment analysis by averaging its\n",
    "    word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Implement computation for the sentence features given a sentence.\n",
    "\n",
    "    # Inputs:\n",
    "    # tokens -- a dictionary that maps words to their indices in\n",
    "    #           the word vector list\n",
    "    # wordVectors -- word vectors (each row) for all tokens\n",
    "    # sentence -- a list of words in the sentence of interest\n",
    "\n",
    "    # Output:\n",
    "    # - sentVector: feature vector for the sentence\n",
    "\n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    for word in sentence:\n",
    "        sentVector += wordVectors[tokens[word], :]\n",
    "\n",
    "    sentVector *= 1.0 / len(sentence)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    assert sentVector.shape == (wordVectors.shape[1],)\n",
    "    return sentVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRegularizationValues():\n",
    "    \"\"\"Try different regularizations\n",
    "\n",
    "    Return a sorted list of values to try.\n",
    "    \"\"\"\n",
    "    values = None   # Assign a list of floats in the block below\n",
    "    ### YOUR CODE HERE\n",
    "    values = np.logspace(-4, 2, num=100, base=10)\n",
    "    ### END YOUR CODE\n",
    "    return sorted(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseBestModel(results):\n",
    "    \"\"\"Choose the best model based on dev set performance.\n",
    "\n",
    "    Arguments:\n",
    "    results -- A list of python dictionaries of the following format:\n",
    "        {\n",
    "            \"reg\": regularization,\n",
    "            \"clf\": classifier,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy\n",
    "        }\n",
    "\n",
    "    Each dictionary represents the performance of one model.\n",
    "\n",
    "    Returns:\n",
    "    Your chosen result dictionary.\n",
    "    \"\"\"\n",
    "    bestResult = None\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    bestResult = max(results, key=lambda x: x[\"dev\"])\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return bestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotRegVsAccuracy(regValues, results, filename):\n",
    "    \"\"\" Make a plot of regularization vs accuracy \"\"\"\n",
    "    plt.plot(regValues, [x[\"train\"] for x in results])\n",
    "    plt.plot(regValues, [x[\"dev\"] for x in results])\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"regularization\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(['train', 'dev'], loc='upper left')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputConfusionMatrix(features, labels, clf, filename):\n",
    "    \"\"\" Generate a confusion matrix \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    cm = confusion_matrix(labels, pred, labels=range(5))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "    plt.colorbar()\n",
    "    classes = [\"- -\", \"-\", \"neut\", \"+\", \"+ +\"]\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputPredictions(dataset, features, labels, clf, filename):\n",
    "    \"\"\" Write the predictions to file \"\"\"\n",
    "    pred = clf.predict(features)\n",
    "    with open(filename, \"w\") as f:\n",
    "        print >> f, \"True\\tPredicted\\tText\"\n",
    "        for i in xrange(len(dataset)):\n",
    "            print >> f, \"%d\\t%d\\t%s\" % (\n",
    "                labels[i], pred[i], \" \".join(dataset[i][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadWordVectors(tokens, filepath=\"utils/datasets/glove.6B.50d.txt\", dimensions=50):\n",
    "    \"\"\"Read pretrained GloVe vectors\"\"\"\n",
    "    wordVectors = np.zeros((len(tokens), dimensions))\n",
    "    with open(filepath) as ifs:\n",
    "        for line in ifs:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            row = line.split()\n",
    "            token = row[0]\n",
    "            if token not in tokens:\n",
    "                continue\n",
    "            data = [float(x) for x in row[1:]]\n",
    "            if len(data) != dimensions:\n",
    "                raise RuntimeError(\"wrong number of dimensions\")\n",
    "            wordVectors[tokens[token]] = np.asarray(data)\n",
    "    return wordVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\" Train a model to do sentiment analyis\"\"\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = StanfordSentiment()\n",
    "    tokens = dataset.tokens()\n",
    "    nWords = len(tokens)\n",
    "\n",
    "#     if args.yourvectors:\n",
    "#         _, wordVectors, _ = load_saved_params()\n",
    "#         wordVectors = np.concatenate(\n",
    "#             (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "#             axis=1)\n",
    "#     elif args.pretrained:\n",
    "#         wordVectors = glove.loadWordVectors(tokens)\n",
    "#     dimVectors = wordVectors.shape[1]\n",
    "    wordVectors = loadWordVectors(tokens)\n",
    "    dimVectors = wordVectors.shape[1]\n",
    "    # Load the train set\n",
    "    trainset = dataset.getTrainSentences()\n",
    "    nTrain = len(trainset)\n",
    "    trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "    trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "    for i in xrange(nTrain):\n",
    "        words, trainLabels[i] = trainset[i]\n",
    "        trainFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare dev set features\n",
    "    devset = dataset.getDevSentences()\n",
    "    nDev = len(devset)\n",
    "    devFeatures = np.zeros((nDev, dimVectors))\n",
    "    devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "    for i in xrange(nDev):\n",
    "        words, devLabels[i] = devset[i]\n",
    "        devFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # Prepare test set features\n",
    "    testset = dataset.getTestSentences()\n",
    "    nTest = len(testset)\n",
    "    testFeatures = np.zeros((nTest, dimVectors))\n",
    "    testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "    for i in xrange(nTest):\n",
    "        words, testLabels[i] = testset[i]\n",
    "        testFeatures[i, :] = getSentenceFeatures(tokens, wordVectors, words)\n",
    "\n",
    "    # We will save our results from each run\n",
    "    results = []\n",
    "    regValues = getRegularizationValues()\n",
    "    for reg in regValues:\n",
    "        print \"Training for reg=%f\" % reg\n",
    "        # Note: add a very small number to regularization to please the library\n",
    "        clf = LogisticRegression(C=1.0/(reg + 1e-12))\n",
    "        clf.fit(trainFeatures, trainLabels)\n",
    "\n",
    "        # Test on train set\n",
    "        pred = clf.predict(trainFeatures)\n",
    "        trainAccuracy = accuracy(trainLabels, pred)\n",
    "        print \"Train accuracy (%%): %f\" % trainAccuracy\n",
    "\n",
    "        # Test on dev set\n",
    "        pred = clf.predict(devFeatures)\n",
    "        devAccuracy = accuracy(devLabels, pred)\n",
    "        print \"Dev accuracy (%%): %f\" % devAccuracy\n",
    "\n",
    "        # Test on test set\n",
    "        # Note: always running on test is poor style. Typically, you should\n",
    "        # do this only after validation.\n",
    "        pred = clf.predict(testFeatures)\n",
    "        testAccuracy = accuracy(testLabels, pred)\n",
    "        print \"Test accuracy (%%): %f\" % testAccuracy\n",
    "\n",
    "        results.append({\n",
    "            \"reg\": reg,\n",
    "            \"clf\": clf,\n",
    "            \"train\": trainAccuracy,\n",
    "            \"dev\": devAccuracy,\n",
    "            \"test\": testAccuracy})\n",
    "\n",
    "    # Print the accuracies\n",
    "    print \"\"\n",
    "    print \"=== Recap ===\"\n",
    "    print \"Reg\\t\\tTrain\\tDev\\tTest\"\n",
    "    for result in results:\n",
    "        print \"%.2E\\t%.3f\\t%.3f\\t%.3f\" % (\n",
    "            result[\"reg\"],\n",
    "            result[\"train\"],\n",
    "            result[\"dev\"],\n",
    "            result[\"test\"])\n",
    "    print \"\"\n",
    "\n",
    "    bestResult = chooseBestModel(results)\n",
    "    print \"Best regularization value: %0.2E\" % bestResult[\"reg\"]\n",
    "    print \"Test accuracy (%%): %f\" % bestResult[\"test\"]\n",
    "\n",
    "    # do some error analysis\n",
    "#     if args.pretrained:\n",
    "    plotRegVsAccuracy(regValues, results, \"q4_reg_v_acc.png\")\n",
    "    outputConfusionMatrix(devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                          \"q4_dev_conf.png\")\n",
    "    outputPredictions(devset, devFeatures, devLabels, bestResult[\"clf\"],\n",
    "                      \"q4_dev_pred.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for reg=0.000100\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.923077\n",
      "Training for reg=0.000115\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.000132\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000152\n",
      "Train accuracy (%): 39.934457\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.104072\n",
      "Training for reg=0.000175\n",
      "Train accuracy (%): 39.852528\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.000201\n",
      "Train accuracy (%): 39.899345\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.000231\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000266\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000305\n",
      "Train accuracy (%): 39.887640\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.000351\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000404\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.000464\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.000534\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.000614\n",
      "Train accuracy (%): 39.875936\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.000705\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.000811\n",
      "Train accuracy (%): 39.899345\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.000933\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.001072\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.001233\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.001417\n",
      "Train accuracy (%): 39.934457\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.001630\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.001874\n",
      "Train accuracy (%): 39.922753\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.002154\n",
      "Train accuracy (%): 39.969569\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 36.968326\n",
      "Training for reg=0.002477\n",
      "Train accuracy (%): 39.969569\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=0.002848\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.003275\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.003765\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.004329\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.004977\n",
      "Train accuracy (%): 39.969569\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=0.005722\n",
      "Train accuracy (%): 39.957865\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.104072\n",
      "Training for reg=0.006579\n",
      "Train accuracy (%): 39.957865\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=0.007565\n",
      "Train accuracy (%): 39.981273\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=0.008697\n",
      "Train accuracy (%): 39.946161\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.010000\n",
      "Train accuracy (%): 39.934457\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.011498\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.013219\n",
      "Train accuracy (%): 39.911049\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.015199\n",
      "Train accuracy (%): 39.899345\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=0.017475\n",
      "Train accuracy (%): 39.887640\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.020092\n",
      "Train accuracy (%): 39.852528\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.023101\n",
      "Train accuracy (%): 39.864232\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.026561\n",
      "Train accuracy (%): 39.840824\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=0.030539\n",
      "Train accuracy (%): 39.840824\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.420814\n",
      "Training for reg=0.035112\n",
      "Train accuracy (%): 39.829120\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.466063\n",
      "Training for reg=0.040370\n",
      "Train accuracy (%): 39.864232\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.420814\n",
      "Training for reg=0.046416\n",
      "Train accuracy (%): 39.875936\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 37.466063\n",
      "Training for reg=0.053367\n",
      "Train accuracy (%): 39.875936\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.420814\n",
      "Training for reg=0.061359\n",
      "Train accuracy (%): 39.864232\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=0.070548\n",
      "Train accuracy (%): 39.864232\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.081113\n",
      "Train accuracy (%): 39.875936\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.093260\n",
      "Train accuracy (%): 39.840824\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.107227\n",
      "Train accuracy (%): 39.794007\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=0.123285\n",
      "Train accuracy (%): 39.782303\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=0.141747\n",
      "Train accuracy (%): 39.782303\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.511312\n",
      "Training for reg=0.162975\n",
      "Train accuracy (%): 39.840824\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.375566\n",
      "Training for reg=0.187382\n",
      "Train accuracy (%): 39.805712\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.375566\n",
      "Training for reg=0.215443\n",
      "Train accuracy (%): 39.723783\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.247708\n",
      "Train accuracy (%): 39.653558\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=0.284804\n",
      "Train accuracy (%): 39.559925\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=0.327455\n",
      "Train accuracy (%): 39.618446\n",
      "Dev accuracy (%): 36.239782\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=0.376494\n",
      "Train accuracy (%): 39.618446\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.432876\n",
      "Train accuracy (%): 39.630150\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=0.497702\n",
      "Train accuracy (%): 39.595037\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=0.572237\n",
      "Train accuracy (%): 39.595037\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=0.657933\n",
      "Train accuracy (%): 39.595037\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=0.756463\n",
      "Train accuracy (%): 39.571629\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=0.869749\n",
      "Train accuracy (%): 39.524813\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.375566\n",
      "Training for reg=1.000000\n",
      "Train accuracy (%): 39.524813\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=1.149757\n",
      "Train accuracy (%): 39.524813\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=1.321941\n",
      "Train accuracy (%): 39.559925\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.375566\n",
      "Training for reg=1.519911\n",
      "Train accuracy (%): 39.489700\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=1.747528\n",
      "Train accuracy (%): 39.477996\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=2.009233\n",
      "Train accuracy (%): 39.501404\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=2.310130\n",
      "Train accuracy (%): 39.419476\n",
      "Dev accuracy (%): 36.603088\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=2.656088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy (%): 39.290730\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=3.053856\n",
      "Train accuracy (%): 39.232210\n",
      "Dev accuracy (%): 37.057221\n",
      "Test accuracy (%): 37.058824\n",
      "Training for reg=3.511192\n",
      "Train accuracy (%): 39.033240\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.239819\n",
      "Training for reg=4.037017\n",
      "Train accuracy (%): 39.103464\n",
      "Dev accuracy (%): 36.784741\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=4.641589\n",
      "Train accuracy (%): 39.009831\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.285068\n",
      "Training for reg=5.336699\n",
      "Train accuracy (%): 38.974719\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.420814\n",
      "Training for reg=6.135907\n",
      "Train accuracy (%): 38.951311\n",
      "Dev accuracy (%): 36.784741\n",
      "Test accuracy (%): 37.466063\n",
      "Training for reg=7.054802\n",
      "Train accuracy (%): 38.916199\n",
      "Dev accuracy (%): 36.966394\n",
      "Test accuracy (%): 37.918552\n",
      "Training for reg=8.111308\n",
      "Train accuracy (%): 38.845974\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 37.828054\n",
      "Training for reg=9.326033\n",
      "Train accuracy (%): 38.600187\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.918552\n",
      "Training for reg=10.722672\n",
      "Train accuracy (%): 38.717228\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.466063\n",
      "Training for reg=12.328467\n",
      "Train accuracy (%): 38.623596\n",
      "Dev accuracy (%): 37.148047\n",
      "Test accuracy (%): 37.556561\n",
      "Training for reg=14.174742\n",
      "Train accuracy (%): 38.448034\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 37.330317\n",
      "Training for reg=16.297508\n",
      "Train accuracy (%): 38.330993\n",
      "Dev accuracy (%): 36.784741\n",
      "Test accuracy (%): 37.194570\n",
      "Training for reg=18.738174\n",
      "Train accuracy (%): 38.190543\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 37.149321\n",
      "Training for reg=21.544347\n",
      "Train accuracy (%): 38.061798\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 37.013575\n",
      "Training for reg=24.770764\n",
      "Train accuracy (%): 37.862828\n",
      "Dev accuracy (%): 36.330609\n",
      "Test accuracy (%): 36.832579\n",
      "Training for reg=28.480359\n",
      "Train accuracy (%): 37.675562\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 36.787330\n",
      "Training for reg=32.745492\n",
      "Train accuracy (%): 37.500000\n",
      "Dev accuracy (%): 36.875568\n",
      "Test accuracy (%): 36.470588\n",
      "Training for reg=37.649358\n",
      "Train accuracy (%): 37.535112\n",
      "Dev accuracy (%): 36.693915\n",
      "Test accuracy (%): 36.334842\n",
      "Training for reg=43.287613\n",
      "Train accuracy (%): 37.382959\n",
      "Dev accuracy (%): 36.512262\n",
      "Test accuracy (%): 36.244344\n",
      "Training for reg=49.770236\n",
      "Train accuracy (%): 37.137172\n",
      "Dev accuracy (%): 36.421435\n",
      "Test accuracy (%): 36.199095\n",
      "Training for reg=57.223677\n",
      "Train accuracy (%): 36.996723\n",
      "Dev accuracy (%): 36.148955\n",
      "Test accuracy (%): 36.063348\n",
      "Training for reg=65.793322\n",
      "Train accuracy (%): 36.844569\n",
      "Dev accuracy (%): 35.694823\n",
      "Test accuracy (%): 36.108597\n",
      "Training for reg=75.646333\n",
      "Train accuracy (%): 36.610487\n",
      "Dev accuracy (%): 35.876476\n",
      "Test accuracy (%): 35.882353\n",
      "Training for reg=86.974900\n",
      "Train accuracy (%): 36.505150\n",
      "Dev accuracy (%): 35.059037\n",
      "Test accuracy (%): 35.610860\n",
      "Training for reg=100.000000\n",
      "Train accuracy (%): 36.329588\n",
      "Dev accuracy (%): 35.059037\n",
      "Test accuracy (%): 35.701357\n",
      "\n",
      "=== Recap ===\n",
      "Reg\t\tTrain\tDev\tTest\n",
      "1.00E-04\t39.923\t36.331\t36.923\n",
      "1.15E-04\t39.946\t36.512\t37.014\n",
      "1.32E-04\t39.923\t36.331\t36.968\n",
      "1.52E-04\t39.934\t36.421\t37.104\n",
      "1.75E-04\t39.853\t36.421\t37.195\n",
      "2.01E-04\t39.899\t36.331\t37.059\n",
      "2.31E-04\t39.911\t36.331\t36.968\n",
      "2.66E-04\t39.911\t36.331\t36.968\n",
      "3.05E-04\t39.888\t36.331\t37.059\n",
      "3.51E-04\t39.911\t36.331\t36.968\n",
      "4.04E-04\t39.911\t36.331\t37.014\n",
      "4.64E-04\t39.946\t36.512\t37.014\n",
      "5.34E-04\t39.923\t36.421\t37.059\n",
      "6.14E-04\t39.876\t36.421\t37.059\n",
      "7.05E-04\t39.946\t36.331\t36.968\n",
      "8.11E-04\t39.899\t36.421\t37.014\n",
      "9.33E-04\t39.923\t36.421\t37.014\n",
      "1.07E-03\t39.911\t36.421\t37.014\n",
      "1.23E-03\t39.911\t36.331\t37.014\n",
      "1.42E-03\t39.934\t36.421\t36.968\n",
      "1.63E-03\t39.923\t36.421\t37.059\n",
      "1.87E-03\t39.923\t36.421\t37.059\n",
      "2.15E-03\t39.970\t36.421\t36.968\n",
      "2.48E-03\t39.970\t36.421\t37.014\n",
      "2.85E-03\t39.946\t36.331\t37.059\n",
      "3.27E-03\t39.911\t36.240\t37.059\n",
      "3.76E-03\t39.946\t36.331\t37.059\n",
      "4.33E-03\t39.946\t36.240\t37.059\n",
      "4.98E-03\t39.970\t36.240\t37.059\n",
      "5.72E-03\t39.958\t36.240\t37.104\n",
      "6.58E-03\t39.958\t36.149\t37.149\n",
      "7.56E-03\t39.981\t36.149\t37.149\n",
      "8.70E-03\t39.946\t36.331\t37.195\n",
      "1.00E-02\t39.934\t36.240\t37.195\n",
      "1.15E-02\t39.911\t36.331\t37.195\n",
      "1.32E-02\t39.911\t36.421\t37.240\n",
      "1.52E-02\t39.899\t36.694\t37.285\n",
      "1.75E-02\t39.888\t36.694\t37.240\n",
      "2.01E-02\t39.853\t36.603\t37.240\n",
      "2.31E-02\t39.864\t36.421\t37.240\n",
      "2.66E-02\t39.841\t36.421\t37.330\n",
      "3.05E-02\t39.841\t36.331\t37.421\n",
      "3.51E-02\t39.829\t36.331\t37.466\n",
      "4.04E-02\t39.864\t36.331\t37.421\n",
      "4.64E-02\t39.876\t36.149\t37.466\n",
      "5.34E-02\t39.876\t36.240\t37.421\n",
      "6.14E-02\t39.864\t36.331\t37.330\n",
      "7.05E-02\t39.864\t36.331\t37.195\n",
      "8.11E-02\t39.876\t36.421\t37.195\n",
      "9.33E-02\t39.841\t36.240\t37.195\n",
      "1.07E-01\t39.794\t36.240\t37.149\n",
      "1.23E-01\t39.782\t36.512\t37.285\n",
      "1.42E-01\t39.782\t36.512\t37.511\n",
      "1.63E-01\t39.841\t36.512\t37.376\n",
      "1.87E-01\t39.806\t36.421\t37.376\n",
      "2.15E-01\t39.724\t36.512\t37.240\n",
      "2.48E-01\t39.654\t36.603\t37.195\n",
      "2.85E-01\t39.560\t36.512\t37.330\n",
      "3.27E-01\t39.618\t36.240\t37.330\n",
      "3.76E-01\t39.618\t36.421\t37.240\n",
      "4.33E-01\t39.630\t36.331\t37.285\n",
      "4.98E-01\t39.595\t36.149\t37.330\n",
      "5.72E-01\t39.595\t36.149\t37.285\n",
      "6.58E-01\t39.595\t36.421\t37.240\n",
      "7.56E-01\t39.572\t36.421\t37.285\n",
      "8.70E-01\t39.525\t36.603\t37.376\n",
      "1.00E+00\t39.525\t36.603\t37.330\n",
      "1.15E+00\t39.525\t36.603\t37.285\n",
      "1.32E+00\t39.560\t36.512\t37.376\n",
      "1.52E+00\t39.490\t36.512\t37.285\n",
      "1.75E+00\t39.478\t36.694\t37.195\n",
      "2.01E+00\t39.501\t36.876\t37.240\n",
      "2.31E+00\t39.419\t36.603\t37.285\n",
      "2.66E+00\t39.291\t36.876\t37.195\n",
      "3.05E+00\t39.232\t37.057\t37.059\n",
      "3.51E+00\t39.033\t36.876\t37.240\n",
      "4.04E+00\t39.103\t36.785\t37.330\n",
      "4.64E+00\t39.010\t36.876\t37.285\n",
      "5.34E+00\t38.975\t36.512\t37.421\n",
      "6.14E+00\t38.951\t36.785\t37.466\n",
      "7.05E+00\t38.916\t36.966\t37.919\n",
      "8.11E+00\t38.846\t36.694\t37.828\n",
      "9.33E+00\t38.600\t36.876\t37.919\n",
      "1.07E+01\t38.717\t36.876\t37.466\n",
      "1.23E+01\t38.624\t37.148\t37.557\n",
      "1.42E+01\t38.448\t36.876\t37.330\n",
      "1.63E+01\t38.331\t36.785\t37.195\n",
      "1.87E+01\t38.191\t36.694\t37.149\n",
      "2.15E+01\t38.062\t36.512\t37.014\n",
      "2.48E+01\t37.863\t36.331\t36.833\n",
      "2.85E+01\t37.676\t36.876\t36.787\n",
      "3.27E+01\t37.500\t36.876\t36.471\n",
      "3.76E+01\t37.535\t36.694\t36.335\n",
      "4.33E+01\t37.383\t36.512\t36.244\n",
      "4.98E+01\t37.137\t36.421\t36.199\n",
      "5.72E+01\t36.997\t36.149\t36.063\n",
      "6.58E+01\t36.845\t35.695\t36.109\n",
      "7.56E+01\t36.610\t35.876\t35.882\n",
      "8.70E+01\t36.505\t35.059\t35.611\n",
      "1.00E+02\t36.330\t35.059\t35.701\n",
      "\n",
      "Best regularization value: 1.23E+01\n",
      "Test accuracy (%): 37.556561\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/24.png\", width=\"500\"><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/23.png\", width=\"500\"><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(с) Alex Yalunin 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
